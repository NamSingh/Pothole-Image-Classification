{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIIA Pothole Image Classification Challenge  \n",
    "\n",
    "This code was developed as part of the [Zindi Image Classification Challenge](https://zindi.africa/competitions/miia-pothole-image-classification-challenge). The goal of this ML project is to reduce the time and money required to repair potholes and improve road infrastructure in South Africa. Through image recognition, we reduce the amount of manual effort needed to identify potholes, enabling faster maintenance and promoting safer roadways.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Import and Setup](#import-and-setup)\n",
    "2. [Cropping Images](#cropping-images)\n",
    "3. [Load the Remaining Data](#load-the-remaining-data)\n",
    "4. [Preprocessing Images (Data Transformations)](#preprocessing-images-data-transformations)\n",
    "5. [Model Definition](#model-definition)\n",
    "6. [Model Training & Evaluation](#model-training-and-evaluation)\n",
    "7. [Model Testing](#model-testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tbnafyq2LNKm"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set a seed for consistent results\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3DFQ5HWLNKl"
   },
   "source": [
    "## Cropping Images\n",
    "\n",
    "We can see that nearly all the images are taken from inside the car, with the dashboard and sky taking up much of the photo. To reduce noise we can crop out irrelevant data, leaving only the road.\n",
    "We have seperated cropping from other data transformations, as we can then focus on a \"new\" dataset which can be easily reused in the future.\n",
    "\n",
    "* To crop the image, make sure you update the path (to the image folder) accordingly.\n",
    "* We can try a variety of ranges for the cropping and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJFNug9ONOPK"
   },
   "outputs": [],
   "source": [
    "import cv2, os, glob\n",
    "\n",
    "# path to original image folder\n",
    "# TODO: Replace with local path to the folder containing the images\n",
    "data_path = r'D:\\MMAI\\MMAI894\\Zindi\\data\\all_data'\n",
    "\n",
    "# path to the folder to save the cropped image\n",
    "save_path = './cropped_data2/'\n",
    "imgfiles = glob.iglob(os.path.join(data_path, '**/*.JPG'), recursive=True)\n",
    "for imgfile in imgfiles:\n",
    "    image = cv2.imread(imgfile)\n",
    "    img_name = imgfile.split('/')[-1]\n",
    "\n",
    "    # Define the coordinates of the top-left and bottom-right points of the cropping rectangle\n",
    "    # Format: (x, y)\n",
    "    top_left = (50, 240)\n",
    "    bottom_right = (750, 430)\n",
    "\n",
    "    # Crop the image using array slicing\n",
    "    # Note: OpenCV images are accessed with (y, x) notation\n",
    "    cropped_image = image[top_left[1]:bottom_right[1], top_left[0]:bottom_right[0]]\n",
    "\n",
    "    # Save or display the cropped image\n",
    "    cv2.imwrite(save_path + img_name, cropped_image)\n",
    "\n",
    "    # or\n",
    "    # cv2.imshow('Cropped Image', cropped_image)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbJylF9ELd_p"
   },
   "source": [
    "## Load the Remaining Data\n",
    "\n",
    "Reminder to update the following paths, for your local machine. In this case we are referencing the cropped dataset, to use the raw dataset please update the path accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PlFsGiZ-ocnn"
   },
   "outputs": [],
   "source": [
    "\n",
    "# change the following path for your own training\n",
    "\n",
    "# path to image folder\n",
    "data_path = r'D:\\MMAI\\MMAI894\\Zindi\\data\\cropped_data1'\n",
    "\n",
    "# path to train csv\n",
    "train_path = r'D:\\MMAI\\MMAI894\\Zindi\\data\\train_ids_labels.csv'\n",
    "\n",
    "# path to test csv\n",
    "test_path = r'D:\\MMAI\\MMAI894\\Zindi\\data\\test_ids_only.csv'\n",
    "\n",
    "# Create a custom dataset class for training and testing\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0] + '.JPG')\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if len(self.annotations.iloc[index]) > 1:\n",
    "            label = int(self.annotations.iloc[index, 1])\n",
    "        else:label = 1\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return (image, label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Rljr29cSpUL"
   },
   "source": [
    "# Preprocessing Images (Data Transformations)\n",
    "\n",
    "Here we can test a variety of transformations. We have commented out several of the transformations as they reduced model quality, but left them listed in as potential parameters. To see a full list of commonly used transfomations, please checkout: https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_illustrations.html#sphx-glr-auto-examples-transforms-plot-transforms-illustrations-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J8mSSd-V4dRM"
   },
   "outputs": [],
   "source": [
    "## Transform the training and validation images\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    # transforms.Resize((800, 600)),\n",
    "    # transforms.RandomCrop((800, 400)),\n",
    "    transforms.Resize((250, 250)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    # transforms.RandomRotation(degrees=(-3, 3), expand=True),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    # transforms.Resize((800, 600)),\n",
    "    # transforms.RandomCrop((800, 400)),\n",
    "    transforms.Resize((250, 250)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    # transforms.RandomRotation(degrees=(-3, 3), expand=True),\n",
    "    # transforms.RandomHorizontalFlip(p=0.5),\n",
    "])\n",
    "\n",
    "# Create an instance of the CustomDataset with no transforms\n",
    "dataset = CustomDataset(csv_file=train_path, root_dir=data_path, transform=None)\n",
    "# Splitting dataset into train and validation\n",
    "train_size = int(0.80 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Wrap the subsets to apply the transformations\n",
    "class TransformWrapper(Dataset):\n",
    "    def __init__(self, dataset, transform):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.dataset[index]\n",
    "        x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "# Apply specific transformations to each subset\n",
    "train_dataset = TransformWrapper(train_dataset, train_transform)\n",
    "val_dataset = TransformWrapper(val_dataset, val_transform)\n",
    "\n",
    "# Create DataLoader for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "In the following code we will leverage ResNet34 a very common CNN (convolutional neural network) for image classification. We have also provided code if you are interested in using the VGG16 CNN. \n",
    "Another model that we have seen success with (though the configuration is not provided here) is MaxVit (Multi-Axis Vision Transformer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet34():\n",
    "    model = models.resnet152(pretrained=True)\n",
    "    # freeze all params\n",
    "    for params in model.parameters():\n",
    "        params.requires_grad_ = False\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=5, stride=2, padding=4, bias=False)\n",
    "    # add a new final layer\n",
    "    nr_filters = model.fc.in_features  # number of input features of last layer\n",
    "    model.fc = nn.Linear(nr_filters, 1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pjAN7uFh9xm0"
   },
   "outputs": [],
   "source": [
    "def vggMod():\n",
    "    # Load the pre-trained VGG16 model\n",
    "    model = models.vgg16(pretrained=True)\n",
    "    \n",
    "    # Freeze all parameters\n",
    "    for params in model.parameters():\n",
    "        params.requires_grad_ = False\n",
    "    \n",
    "    # Add a Conv2d layer with 64 output channels, kernel_size=3, and padding=1\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=5, stride=2, padding=4, bias=False)\n",
    "    \n",
    "    # Add a pooling layer (you can use MaxPooling or AveragePooling)\n",
    "    model.pool1  = nn.MaxPool2d(kernel_size=5, stride=2)\n",
    "    \n",
    "    # Modify the classifier to have 1 output neuron\n",
    "    nr_features = model.classifier[-1].in_features\n",
    "    model.classifier[-1] = nn.Linear(nr_features, 1)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KyV4aw4N93ZU",
    "outputId": "1da7f9e5-9d4b-4c14-88ab-24eaca058043"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Namsi\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Namsi\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = resnet34()\n",
    "# model = vggMod()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=6, eta_min=0.00002)\n",
    "\n",
    "def evaluate(model, device, val_loader, criterion):\n",
    "    model.eval()\n",
    "    final_outputs = []\n",
    "    final_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            final_outputs.extend(outputs.sigmoid().cpu().detach().numpy())\n",
    "            final_targets.extend(labels.cpu().detach().numpy())\n",
    "\n",
    "    # Calculate AUC\n",
    "    auc = roc_auc_score(final_targets, final_outputs)\n",
    "    return auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jz3uTEmuZ21y"
   },
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "In addition to training the model, we will save a version of the model (each epoch) with the best validation accuracy. During training watch for losses that are too small, the model may be overfitting.\n",
    "\n",
    "Note: \n",
    "- Please change the epochs as necessary, taking into account both training time and resulting performance.\n",
    "- Feel free to update the saved \"Model Name\", as to avoid overwriting previous iterations and experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "OOuMLPw2_cgg",
    "outputId": "94f8504a-749e-4636-86d0-ff2c536e0d4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 25.0141, Val. AUC: 0.9801\n",
      "Epoch: 2, Train Loss: 42.7203, Val. AUC: 0.9753\n",
      "Epoch: 3, Train Loss: 24.9684, Val. AUC: 0.9767\n",
      "Epoch: 4, Train Loss: 15.3840, Val. AUC: 0.9871\n",
      "Epoch: 5, Train Loss: 9.4232, Val. AUC: 0.9913\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "model.train()\n",
    "best_auc, best_epoch, best_model = 0, 0, model\n",
    "\n",
    "for epoch in range(20):\n",
    "    epoch_loss = 0\n",
    "\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs).squeeze()\n",
    "\n",
    "        # Ensure outputs and labels are correctly shaped\n",
    "        labels = labels.float()  # Ensure labels are float for BCELoss\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Evaluate the model after each epoch\n",
    "    val_auc = evaluate(model, device, val_loader, criterion)\n",
    "    scheduler.step()\n",
    "    if val_auc > best_auc:\n",
    "        best_auc = val_auc\n",
    "        best_model = model\n",
    "        best_epoch = epoch + 1\n",
    "\n",
    "    # Print the results for each epoch\n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {epoch_loss:.4f}, Val. AUC: {val_auc:.4f}')\n",
    "\n",
    "# Save the best model\n",
    "torch.save(best_model.state_dict(), 'BestModel_' + str(best_epoch) + '.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4Xrel0QaKK1"
   },
   "source": [
    "## Model Testing\n",
    "\n",
    "Test the model (against the test dataset), the result of this cell will be an excel spreadsheet that can be submitted to the [Zindi Competition](https://zindi.africa/competitions/miia-pothole-image-classification-challenge/submit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pSFK6sZ1TUt9"
   },
   "outputs": [],
   "source": [
    "# Function for testing the model\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            # Get probabilities for the positive class\n",
    "            probs = outputs.sigmoid().cpu().numpy()\n",
    "            test_probs.extend(probs.squeeze())\n",
    "\n",
    "    return test_probs\n",
    "\n",
    "# Load the Test DataSet\n",
    "test_dataset = CustomDataset(csv_file=test_path, root_dir=data_path, transform=val_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Test the model\n",
    "test_probs = test(best_model, device, test_loader)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "test_images = [test_dataset.annotations.iloc[idx, 0] for idx in range(len(test_dataset))]\n",
    "test_results = pd.DataFrame({'Image_ID': test_images, 'Label': test_probs})\n",
    "test_results.to_csv(r'D:\\MMAI\\MMAI894\\Zindi\\out\\test-results-01.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
